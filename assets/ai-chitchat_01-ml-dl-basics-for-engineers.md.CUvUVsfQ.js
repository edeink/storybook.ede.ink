import{f as r,D as n,c as o,o as s,k as e,a4 as i,H as d,a as l}from"./chunks/framework.BAs4fWuZ.js";const f=JSON.parse('{"title":"AI 碎碎念 01：机器学习与深度学习的浅尝即止（工程师够用版）","description":"","frontmatter":{"title":"AI 碎碎念 01：机器学习与深度学习的浅尝即止（工程师够用版）","date":"2026-01-21T00:00:00.000Z","categories":"study","tags":["ai-chitchat","ml","dl","backprop","gradient-descent"]},"headers":[],"relativePath":"ai-chitchat/01-ml-dl-basics-for-engineers.md","filePath":"ai-chitchat/01-ml-dl-basics-for-engineers.md","lastUpdated":1769026429000}'),h={name:"ai-chitchat/01-ml-dl-basics-for-engineers.md"};function p(c,a,u,b,_,g){const t=n("InkMermaidBlock");return s(),o("div",null,[a[0]||(a[0]=e("div",{class:"alert read-stats",role:"note"},[e("span",{class:"read-stats__icon","aria-hidden":"true"},"🔖"),e("span",{class:"read-stats__text"},[l("本文约 "),e("b",null,"1473"),l(" 字，阅读预计耗时 "),e("b",null,"4"),l(" 分钟。")])],-1)),a[1]||(a[1]=e("div",{class:"alert ai-disclosure",role:"note"},[e("span",{class:"ai-disclosure__icon","aria-hidden":"true"},"ⓘ"),e("span",{class:"ai-disclosure__text"},"AI 辅助写作声明：本文由博主构思逻辑，AI 辅助润色，双方共同校对。")],-1)),a[2]||(a[2]=i('<p>我从 2023 年开始陆陆续续学 AI，2024 年在电商里下场折腾。但如果从完整 AI 体系看，我依旧是“勺子”水平：连半桶水都算不上。</p><p>所以这篇不装专业，也不试图把你推去当算法研究员。我只想把几个绕不开的概念讲成“工程直觉”：你听完能知道它大概在干什么、贵在哪里、会在哪些地方翻车。</p><h2 id="_1-先把-ai-到底在干什么-说清楚" tabindex="-1">1. 先把“AI 到底在干什么”说清楚 <a class="header-anchor" href="#_1-先把-ai-到底在干什么-说清楚" aria-label="Permalink to &quot;1. 先把“AI 到底在干什么”说清楚&quot;">​</a></h2><p>多数你今天看到的 AI（尤其是深度学习）都可以被压缩成一个句子：</p><ul><li>你给一堆输入 x（图、文、特征）</li><li>你希望它输出 y（分类、回归、下一 token、噪声残差）</li><li>你定义一个损失函数 L(ŷ, y) 来衡量“错多少”</li><li>你用梯度下降把参数往“让 L 变小”的方向挪</li></ul><p>工程上，AI 的核心并不神秘：它是一个可训练的函数近似器。它强在表达能力与规模，弱在可解释性与稳定性。</p>',6)),d(t,{codeBase64:"Zmxvd2NoYXJ0IExSCiAgRFvmlbDmja4geCx5XSAtLT4gTVvmqKHlnosgZih4OyDOuCldCiAgTSAtLT4gWUhb6aKE5rWLIHnMgl0KICBZSCAtLT4gTFvmjZ/lpLEgTCh5zIIseSldCiAgTCAtLT4gR1vmoq/luqYg4oiCTC/iiILOuF0KICBHIC0tPiBVW+abtOaWsCDOuF0KICBVIC0tPiBNCg=="}),a[3]||(a[3]=i('<h2 id="_2-传统机器学习-vs-深度学习-差别到底在哪" tabindex="-1">2. 传统机器学习 vs 深度学习：差别到底在哪 <a class="header-anchor" href="#_2-传统机器学习-vs-深度学习-差别到底在哪" aria-label="Permalink to &quot;2. 传统机器学习 vs 深度学习：差别到底在哪&quot;">​</a></h2><p>我粗暴地用一句话区分：</p><ul><li>传统机器学习：你负责把“世界”变成特征，模型负责在特征空间里拟合</li><li>深度学习：你把更多“特征提取的工作”也交给模型，但代价是数据与算力需求暴涨</li></ul><p>所以传统路线里，你经常会听到“特征工程”；而深度学习里，你更常听到“数据规模、架构、训练稳定性”。</p><p>这不意味着传统 ML 过时。很多业务问题（结构化数据、强规则场景、小样本、强可解释）仍然很适合树模型/线性模型。工程里的现实是：能跑起来、能解释、能维护、能上线，往往比“论文 SOTA”更重要。</p><h2 id="_3-梯度下降-为什么-能学-最终都落在优化" tabindex="-1">3. 梯度下降：为什么“能学”最终都落在优化 <a class="header-anchor" href="#_3-梯度下降-为什么-能学-最终都落在优化" aria-label="Permalink to &quot;3. 梯度下降：为什么“能学”最终都落在优化&quot;">​</a></h2><p>梯度下降的直觉可以用一个很土的类比：</p><ul><li>损失函数像地形高度</li><li>你站在山上想下到谷底</li><li>梯度就是“往哪边下坡最快”</li></ul><p>于是每一步你做的事是：</p><p>θ ← θ - η · ∇θ L</p><p>这里的 η（学习率）就很像“步子迈多大”。步子太大容易摔死（发散），步子太小容易磨蹭到天荒地老（收敛慢）。</p><p>工程里，优化问题经常不是“会不会算梯度”，而是：</p><ul><li>你的损失定义对不对（是否真的在优化你想要的指标）</li><li>你的数据分布对不对（是否带偏、是否泄漏、是否和线上一致）</li><li>你的训练是否稳定（数值爆炸、梯度消失、训练发散）</li></ul><h2 id="_4-反向传播-它不是玄学-只是高效算梯度" tabindex="-1">4. 反向传播：它不是玄学，只是高效算梯度 <a class="header-anchor" href="#_4-反向传播-它不是玄学-只是高效算梯度" aria-label="Permalink to &quot;4. 反向传播：它不是玄学，只是高效算梯度&quot;">​</a></h2><p>反向传播（Backpropagation）本质是链式法则的工程化：把一个大函数拆成很多层，每层都能算“输入对输出的导数”，然后把梯度从后往前传。</p><p>你可以把它理解成“对账”：</p><ul><li>前向：算出结果，得到“欠账”（损失）</li><li>反向：把欠账按责任分摊到每一层参数上</li></ul><p>所以它不是“AI 的秘密武器”，它只是让“可微分的模型”能被大规模训练。</p><h2 id="_5-激活函数-为什么非线性是必要的" tabindex="-1">5. 激活函数：为什么非线性是必要的 <a class="header-anchor" href="#_5-激活函数-为什么非线性是必要的" aria-label="Permalink to &quot;5. 激活函数：为什么非线性是必要的&quot;">​</a></h2><p>没有非线性，很多层线性变换叠起来，仍然等价于一层线性变换——表达能力上不去。</p><p>激活函数（ReLU、GELU、Sigmoid 等）就是给网络塞进非线性，代价是训练的数值性质会变复杂，出现“梯度消失/爆炸”等问题。你不需要背每个激活函数的公式，但你要知道它们在工程上影响的是：</p><ul><li>训练稳定性</li><li>收敛速度</li><li>表达能力与可拟合性</li></ul><h2 id="_6-三个常见工程概念-batch、embedding、latent" tabindex="-1">6. 三个常见工程概念：batch、embedding、latent <a class="header-anchor" href="#_6-三个常见工程概念-batch、embedding、latent" aria-label="Permalink to &quot;6. 三个常见工程概念：batch、embedding、latent&quot;">​</a></h2><h3 id="_6-1-batching-吞吐和延迟的互相折磨" tabindex="-1">6.1 Batching：吞吐和延迟的互相折磨 <a class="header-anchor" href="#_6-1-batching-吞吐和延迟的互相折磨" aria-label="Permalink to &quot;6.1 Batching：吞吐和延迟的互相折磨&quot;">​</a></h3><p>batching 的直觉很简单：一次算一条太浪费 GPU，于是把多条拼成一批一起算。</p><p>但它也会带来副作用：</p><ul><li>batch 越大，吞吐越高；但单个请求等得越久（延迟上升）</li><li>batch 越大，显存越吃紧；而显存不够就是一票否决</li></ul><p>所以在推理侧，batching 经常不是“越大越好”，而是“在延迟预算内尽量大”。</p><h3 id="_6-2-embedding-把离散东西变成可计算的向量" tabindex="-1">6.2 Embedding：把离散东西变成可计算的向量 <a class="header-anchor" href="#_6-2-embedding-把离散东西变成可计算的向量" aria-label="Permalink to &quot;6.2 Embedding：把离散东西变成可计算的向量&quot;">​</a></h3><p>文字、类目、用户、商品 ID 都是离散符号，模型没法直接算。embedding 做的事就是：把离散符号映射到一个向量空间里，让“相似性”有机会被学习出来。</p><p>工程上你只需要记住一句话：embedding 是模型的“词典”，坏 embedding 会让模型一开始就听不懂你在说什么。</p><h3 id="_6-3-latent-压缩后的表达空间" tabindex="-1">6.3 Latent：压缩后的表达空间 <a class="header-anchor" href="#_6-3-latent-压缩后的表达空间" aria-label="Permalink to &quot;6.3 Latent：压缩后的表达空间&quot;">​</a></h3><p>latent space 可以理解成一种“更小、更结构化的压缩表示”。它常见于生成模型：先把像素压进 latent，再在 latent 里做复杂计算（更便宜），最后再解码回像素。</p><p>你后面会在扩散模型里反复见到它。</p><h2 id="_7-失败场景与排障视角-工程人更关心这个" tabindex="-1">7. 失败场景与排障视角（工程人更关心这个） <a class="header-anchor" href="#_7-失败场景与排障视角-工程人更关心这个" aria-label="Permalink to &quot;7. 失败场景与排障视角（工程人更关心这个）&quot;">​</a></h2><ul><li>过拟合：训练集很好，线上很烂；常见原因是数据量不足、标签噪声、模型太大</li><li>数据泄漏：训练时偷偷看到了未来信息，导致评估虚高，上线即翻车</li><li>分布漂移：训练数据和线上数据不一致；模型不是“坏了”，而是“被换了世界”</li><li>指标错配：你优化的损失和业务指标不是同一个东西</li></ul><p>排障的第一步通常不是“换个更大模型”，而是把口径钉死：</p><ul><li>数据怎么来的</li><li>标签怎么算的</li><li>评估集是否代表线上</li><li>失败样本有没有被系统性覆盖</li></ul><h2 id="参考" tabindex="-1">参考 <a class="header-anchor" href="#参考" aria-label="Permalink to &quot;参考&quot;">​</a></h2><ul><li>李宏毅机器学习课程（课程主页）<a href="https://speech.ee.ntu.edu.tw/~hylee/ml/" target="_blank" rel="noreferrer">https://speech.ee.ntu.edu.tw/~hylee/ml/</a> ；YouTube 频道也可直接搜</li><li>Deep Learning（Goodfellow, Bengio, Courville）<a href="https://www.deeplearningbook.org/" target="_blank" rel="noreferrer">https://www.deeplearningbook.org/</a></li><li>Rumelhart, Hinton, Williams. Learning representations by back-propagating errors (1986) <a href="https://www.nature.com/articles/323533a0" target="_blank" rel="noreferrer">https://www.nature.com/articles/323533a0</a></li></ul>',40))])}const I=r(h,[["render",p]]);export{f as __pageData,I as default};
