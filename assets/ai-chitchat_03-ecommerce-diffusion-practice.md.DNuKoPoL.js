import{_ as n,C as o,c as p,o as t,j as e,a2 as l,b as d,a as i,w as r,G as E,a3 as u}from"./chunks/framework.C2eRlmqf.js";const D=JSON.parse('{"title":"AI 碎碎念 03：电商里的扩散模型折腾记（Mask、Inpaint、Outpaint 到虚拟试衣）","description":"","frontmatter":{"title":"AI 碎碎念 03：电商里的扩散模型折腾记（Mask、Inpaint、Outpaint 到虚拟试衣）","date":"2026-01-21T00:00:00.000Z","categories":"study","tags":["ai-chitchat","diffusion","inpaint","outpaint","controlnet","ecommerce","try-on"]},"headers":[],"relativePath":"ai-chitchat/03-ecommerce-diffusion-practice.md","filePath":"ai-chitchat/03-ecommerce-diffusion-practice.md","lastUpdated":1769026429000}'),h={name:"ai-chitchat/03-ecommerce-diffusion-practice.md"};function c(_,a,A,m,B,f){const s=o("Mermaid");return t(),p("div",null,[a[1]||(a[1]=e("div",{class:"alert read-stats",role:"note"},[e("span",{class:"read-stats__icon","aria-hidden":"true"},"🔖"),e("span",{class:"read-stats__text"},[i("本文约 "),e("b",null,"1119"),i(" 字，阅读预计耗时 "),e("b",null,"3"),i(" 分钟。")])],-1)),a[2]||(a[2]=e("div",{class:"alert ai-disclosure",role:"note"},[e("span",{class:"ai-disclosure__icon","aria-hidden":"true"},"ⓘ"),e("span",{class:"ai-disclosure__text"},"AI 辅助写作声明：本文由博主构思逻辑，AI 辅助润色，双方共同校对。")],-1)),a[3]||(a[3]=l('<p>第二篇讲的是扩散模型的“故事”。这篇讲“落地”：我在电商里折腾扩散模型时，真正卡人的往往不是 DDPM 的公式，而是参数、约束、数据和评估口径。</p><p>这篇会尽量保留工程语气：不讲玄学 prompt，不吹“AI 改变世界”，只讲“管线怎么搭、哪里会翻车、怎么兜底”。</p><h2 id="_1-先说结论-电商里生成的难点不在-能不能画" tabindex="-1">1. 先说结论：电商里生成的难点不在“能不能画” <a class="header-anchor" href="#_1-先说结论-电商里生成的难点不在-能不能画" aria-label="Permalink to &quot;1. 先说结论：电商里生成的难点不在“能不能画”&quot;">​</a></h2><p>电商生成的难点经常是：</p><ul><li>不是“能不能生成”，而是“能不能稳定生成同一种风格、同一种口径、同一种约束”</li><li>不是“单张图好看”，而是“规模化下可控、可回归、可解释”</li><li>不是“离线 demo”，而是“线上成本、延迟、失败率、合规”一起过关</li></ul><h2 id="_2-mask-inpaint-outpaint-从-生成-走向-编辑" tabindex="-1">2. Mask / Inpaint / Outpaint：从“生成”走向“编辑” <a class="header-anchor" href="#_2-mask-inpaint-outpaint-从-生成-走向-编辑" aria-label="Permalink to &quot;2. Mask / Inpaint / Outpaint：从“生成”走向“编辑”&quot;">​</a></h2><h3 id="_2-1-mask-先把编辑边界说清楚" tabindex="-1">2.1 Mask：先把编辑边界说清楚 <a class="header-anchor" href="#_2-1-mask-先把编辑边界说清楚" aria-label="Permalink to &quot;2.1 Mask：先把编辑边界说清楚&quot;">​</a></h3><p>在工程上，Mask 不是一个像素图那么简单，它是一个契约：</p><ul><li>哪些区域允许改</li><li>哪些区域必须保持</li><li>边界怎么融合（羽化、过渡、色彩一致性）</li></ul><h3 id="_2-2-inpaint-在原图上-补" tabindex="-1">2.2 Inpaint：在原图上“补” <a class="header-anchor" href="#_2-2-inpaint-在原图上-补" aria-label="Permalink to &quot;2.2 Inpaint：在原图上“补”&quot;">​</a></h3><p>Inpaint 很像“修复老照片”：你告诉模型缺失区域是什么，它在保持上下文一致的前提下补全。</p><p>常见翻车点：</p><ul><li>边界不一致（明暗、材质、透视）</li><li>语义漂移（补出来的东西不是你要的商品/元素）</li></ul><h3 id="_2-3-outpaint-把画面-扩出去" tabindex="-1">2.3 Outpaint：把画面“扩出去” <a class="header-anchor" href="#_2-3-outpaint-把画面-扩出去" aria-label="Permalink to &quot;2.3 Outpaint：把画面“扩出去”&quot;">​</a></h3><p>Outpaint 更像“扩展画布”：把边缘往外拓展，让画面更完整。</p><p>常见翻车点：</p><ul><li>内容继续性差（纹理、背景不连贯）</li><li>画面主体被“改味儿”</li></ul><h2 id="_3-商品详描图实验-figma-→-json-→-sd-填坑" tabindex="-1">3. 商品详描图实验：Figma → JSON → SD 填坑 <a class="header-anchor" href="#_3-商品详描图实验-figma-→-json-→-sd-填坑" aria-label="Permalink to &quot;3. 商品详描图实验：Figma → JSON → SD 填坑&quot;">​</a></h2><p>我做过一类相对工程化的尝试：把“设计稿”拆成结构化模板，让生成模型只负责填坑位。</p><ul><li>输入：Figma 设计稿</li><li>中间表示：模板 JSON（组件、坐标、坑位、约束）</li><li>输出：由 SD 生成图/文素材，再回填到模板里</li></ul><p>这条链路的价值在于：你把“可控性”从 prompt 转移到“结构与约束”上——工程上更容易治理。</p><p>但它也有很现实的上限：</p><ul><li>布局质量：纯生成难以稳定符合设计系统的细粒度规范</li><li>评估口径：你需要定义“哪些差异可以接受”，否则无法做规模化回归</li></ul><p>我也做过布局方向的探索，但效果不理想。回头看并不意外：布局本质是强约束优化问题，数据分布、规则密度、验收口径都决定了它很难被“纯生成”稳定解决。</p><h2 id="_4-nano-的降维打击-为什么电商不适合做某些方向" tabindex="-1">4. “Nano 的降维打击”：为什么电商不适合做某些方向 <a class="header-anchor" href="#_4-nano-的降维打击-为什么电商不适合做某些方向" aria-label="Permalink to &quot;4. “Nano 的降维打击”：为什么电商不适合做某些方向&quot;">​</a></h2><p>我后来遇到过类似“外部能力突然跃迁”的冲击：当更强的通用模型出现时，一些依赖“细节调参/管线拼装”的局部优势会被迅速抹平。</p><p>这不意味着工程白做了。反而说明：</p><ul><li>你要尽早识别哪些是“可被通用模型吞掉”的部分</li><li>把精力放在更难被吞掉的部分：数据、约束、产品化、评估、成本治理、合规</li></ul><h2 id="_5-虚拟试衣-dwpose-openpose-densepose-3d-建模的拼装管线" tabindex="-1">5. 虚拟试衣：dwpose/openpose + densepose + 3D 建模的拼装管线 <a class="header-anchor" href="#_5-虚拟试衣-dwpose-openpose-densepose-3d-建模的拼装管线" aria-label="Permalink to &quot;5. 虚拟试衣：dwpose/openpose + densepose + 3D 建模的拼装管线&quot;">​</a></h2><p>虚拟试衣是另一类“生成 + 约束”的问题：你不仅要好看，还要符合人体姿态、衣物形变、遮挡与一致性。</p><p>一个常见的工程拼装思路是：先把几何/姿态约束做出来，再把生成当作“渲染器/细化器”。</p>',31)),(t(),d(u,null,{default:r(()=>[E(s,{id:"mermaid-173",class:"mermaid",graph:"flowchart%20TB%0A%20%20I%5B%E8%BE%93%E5%85%A5%EF%BC%9A%E4%BA%BA%E7%89%A9%E5%9B%BE%2F%E8%A7%86%E9%A2%91%5D%20--%3E%20P%5B%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1%3Cbr%2F%3EOpenPose%2FDWPose%5D%0A%20%20I%20--%3E%20D%5B%E4%BA%BA%E4%BD%93%E5%AF%86%E9%9B%86%E5%AF%B9%E9%BD%90%3Cbr%2F%3EDensePose%5D%0A%20%20P%20--%3E%20M%5B%E5%87%A0%E4%BD%95%2F%E9%AA%A8%E6%9E%B6%E7%BA%A6%E6%9D%9F%5D%0A%20%20D%20--%3E%20M%0A%20%20M%20--%3E%20R%5B3D%20%E4%BB%A3%E7%90%86%2F%E5%BB%BA%E6%A8%A1%3Cbr%2F%3E%E7%B2%97%E5%87%A0%E4%BD%95%E4%B8%8E%E9%81%AE%E6%8C%A1%5D%0A%20%20R%20--%3E%20G%5B%E7%94%9F%E6%88%90%2F%E7%BB%86%E5%8C%96%3Cbr%2F%3E%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%5D%0A%20%20G%20--%3E%20O%5B%E8%BE%93%E5%87%BA%EF%BC%9A%E8%AF%95%E8%A1%A3%E6%95%88%E6%9E%9C%E5%9B%BE%2F%E8%A7%86%E9%A2%91%5D%0A"})]),fallback:r(()=>[...a[0]||(a[0]=[i(" Loading... ",-1)])]),_:1})),a[4]||(a[4]=l('<p>常见翻车点：</p><ul><li>姿态估计错误 → 直接导致全链路崩</li><li>遮挡处理不稳定 → “穿帮”非常明显</li><li>数据与评估困难 → “好不好看”太主观，需要可量化口径拆分（肢体对齐/衣物边界/纹理一致性/面部稳定）</li></ul><h2 id="_6-前端视角-关键是把-参数-做成产品能力" tabindex="-1">6. 前端视角：关键是把“参数”做成产品能力 <a class="header-anchor" href="#_6-前端视角-关键是把-参数-做成产品能力" aria-label="Permalink to &quot;6. 前端视角：关键是把“参数”做成产品能力&quot;">​</a></h2><p>在这些落地里，我越来越确信：前端的关键不是“会不会训练模型”，而是把算法能力包装成可用的产品能力。</p><ul><li>把输入约束说清楚（格式、边界、失败场景）</li><li>把参数暴露得足够灵活（不是只给后端一个 endpoint）</li><li>把可观测与回归做起来（失败样本、版本对齐、对账）</li></ul><p>你可以把这件事理解成：前端在做“交互与契约”，而不是“画图”。</p><h2 id="参考" tabindex="-1">参考 <a class="header-anchor" href="#参考" aria-label="Permalink to &quot;参考&quot;">​</a></h2><ul><li>Stable Diffusion（LDM）<a href="https://arxiv.org/abs/2112.10752" target="_blank" rel="noreferrer">https://arxiv.org/abs/2112.10752</a></li><li>ControlNet <a href="https://arxiv.org/abs/2302.05543" target="_blank" rel="noreferrer">https://arxiv.org/abs/2302.05543</a></li><li>OpenPose: Cao et al. Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields (2017) <a href="https://arxiv.org/abs/1611.08050" target="_blank" rel="noreferrer">https://arxiv.org/abs/1611.08050</a></li><li>DensePose: Güler et al. DensePose: Dense Human Pose Estimation In The Wild (2018) <a href="https://arxiv.org/abs/1802.00434" target="_blank" rel="noreferrer">https://arxiv.org/abs/1802.00434</a></li></ul>',8))])}const P=n(h,[["render",c]]);export{D as __pageData,P as default};
