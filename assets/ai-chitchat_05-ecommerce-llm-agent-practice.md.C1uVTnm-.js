import{f as i,D as r,c as n,o,k as a,H as d,a4 as s,a as l}from"./chunks/framework.BAs4fWuZ.js";const I=JSON.parse('{"title":"AI 碎碎念 05：电商里的 LLM 落地（Agent、Prompt 与 A2UI）","description":"","frontmatter":{"title":"AI 碎碎念 05：电商里的 LLM 落地（Agent、Prompt 与 A2UI）","date":"2026-01-21T00:00:00.000Z","categories":"study","tags":["ai-chitchat","llm","agent","prompt","langgraph","xui"]},"headers":[],"relativePath":"ai-chitchat/05-ecommerce-llm-agent-practice.md","filePath":"ai-chitchat/05-ecommerce-llm-agent-practice.md","lastUpdated":1769193553000}'),p={name:"ai-chitchat/05-ecommerce-llm-agent-practice.md"};function h(c,t,u,m,g,L){const e=r("InkMermaidBlock");return o(),n("div",null,[t[0]||(t[0]=a("div",{class:"alert read-stats",role:"note"},[a("span",{class:"read-stats__icon","aria-hidden":"true"},"🔖"),a("span",{class:"read-stats__text"},[l("本文约 "),a("b",null,"1320"),l(" 字，阅读预计耗时 "),a("b",null,"4"),l(" 分钟。")])],-1)),t[1]||(t[1]=a("div",{class:"alert ai-disclosure",role:"note"},[a("span",{class:"ai-disclosure__icon","aria-hidden":"true"},"ⓘ"),a("span",{class:"ai-disclosure__text"},"AI 辅助写作声明：本文由博主构思逻辑，AI 辅助润色，双方共同校对。")],-1)),t[2]||(t[2]=a("p",null,"如果说 LLM 的原理部分更多是“知道它为什么看起来聪明”，那落地部分就是“接受它默认不可靠”，然后用工程机制把它变得可控。",-1)),t[3]||(t[3]=a("p",null,"这篇我只写我在电商里更接近现实的一面：多数时候我们不是训练模型，而是在调用 API；难点不在“让它说话”，而在“让它按你想要的格式说对话、说稳定、可回归”。",-1)),t[4]||(t[4]=a("h2",{id:"_1-agent-的最小闭环-规划-执行-复核",tabindex:"-1"},[l("1. Agent 的最小闭环：规划 / 执行 / 复核 "),a("a",{class:"header-anchor",href:"#_1-agent-的最小闭环-规划-执行-复核","aria-label":'Permalink to "1. Agent 的最小闭环：规划 / 执行 / 复核"'},"​")],-1)),t[5]||(t[5]=a("p",null,"我更认可的 Agent 形态是“最小闭环”，而不是“自嗨式自动化”：",-1)),d(e,{codeBase64:"c3RhdGVEaWFncmFtLXYyCiAgWypdIC0tPiBQbGFuOiDop4TliJI8YnIvPuaLhuS7u+WKoS/lrprovpPlh7rmoLzlvI8KICBQbGFuIC0tPiBBY3Q6IOaJp+ihjDxici8+5bel5YW36LCD55SoL+ajgOe0oi/nlJ/miJAKICBBY3QgLS0+IFZlcmlmeTog5aSN5qC4PGJyLz7nu5PmnoTmoKHpqowv57qm5p2f5qOA5p+lL+iHquajgAogIFZlcmlmeSAtLT4gRG9uZTog6YCa6L+HCiAgVmVyaWZ5IC0tPiBQbGFuOiDkuI3pgJrov4c8YnIvPuihpeS/oeaBry/mlLnorqHliJIKICBEb25lIC0tPiBbKl0K"}),t[6]||(t[6]=s('<p>工程要点：</p><ul><li>规划阶段必须把“验收标准”写出来，否则后面都是玄学</li><li>执行阶段必须收敛工具数量与失败处理，否则成本和不确定性会指数增长</li><li>复核阶段必须可解释：为什么通过、为什么失败、失败如何回放</li></ul><h2 id="_2-智能体-agent-是什么-从-会说话-到-能做事" tabindex="-1">2. 智能体（Agent）是什么：从“会说话”到“能做事” <a class="header-anchor" href="#_2-智能体-agent-是什么-从-会说话-到-能做事" aria-label="Permalink to &quot;2. 智能体（Agent）是什么：从“会说话”到“能做事”&quot;">​</a></h2><p>我这里说的 Agent（智能体），不是某个框架、也不是“自动化脚本 + LLM”的炫技版，而是一种能力组合：它能把输入信号（感知）变成可执行动作，并且对结果负责（可复核、可回放、可追责）。</p><p>如果把它拆成最小模块，我更愿意用这几个词对齐：</p><ul><li>感知：读懂用户意图 + 读懂环境状态（上下文、库存、价格、规则、风控信号）</li><li>行动：能调用工具把世界“改掉”（查/算/写/下发），而不只是生成一段话</li><li>推理与决策：能在多约束下选路径（收益、成本、风险、时延），并解释为什么这么选</li><li>记忆与反思：能在多轮交互里保持一致性，能从失败样本里把坑沉淀成约束</li></ul><h3 id="_2-1-电商里的一种能力分级-l1-l5" tabindex="-1">2.1 电商里的一种能力分级（L1-L5） <a class="header-anchor" href="#_2-1-电商里的一种能力分级-l1-l5" aria-label="Permalink to &quot;2.1 电商里的一种能力分级（L1-L5）&quot;">​</a></h3><p>我在电商场景里更常用的是“能力分级”的视角：不是说模型有多聪明，而是说系统敢把多大权限交给它、以及交付物能不能被工程化兜住。</p><table tabindex="0"><thead><tr><th>Level</th><th>核心模块</th><th>典型形态</th><th>你必须先做的工程门禁</th></tr></thead><tbody><tr><td>L1</td><td>规则（感知 + 行动）</td><td>规则驱动的工具调用与流程编排</td><td>schema 校验、权限收敛、失败可回放</td></tr><tr><td>L2</td><td>L1 + 推理与决策</td><td>多路径选择、策略选择、约束权衡</td><td>约束显式化、决策可解释、离线评测集</td></tr><tr><td>L3</td><td>L2 + 记忆与反思</td><td>多轮一致、长期任务跟进、失败归因与自我修正</td><td>记忆分层（短/长）、反思写回、回归用例沉淀</td></tr><tr><td>L4</td><td>L3 + 自主学习与泛化</td><td>把新规则/新类目迁移到新任务，形成可复用策略</td><td>学习边界与安全阀、A/B 与审计、退化与回滚</td></tr><tr><td>L5</td><td>L4 + 多 Agent 协作 + 情感/个性</td><td>角色分工的“车队”，对外呈现稳定人格与风格</td><td>协议与仲裁、冲突检测、成本预算与配额</td></tr></tbody></table><p>换句话说：L1-L3 解决“能用且可控”，L4 追求“越用越省心”，L5 追求“规模化协作与一致体验”。但级别越高，核心矛盾越明显：你要么把权限收紧换可控，要么把权限放开换效率，而电商场景通常更愿意为可控付费。</p><h2 id="_3-prompt-我更关心-约束与可复现-不关心花活" tabindex="-1">3. Prompt：我更关心“约束与可复现”，不关心花活 <a class="header-anchor" href="#_3-prompt-我更关心-约束与可复现-不关心花活" aria-label="Permalink to &quot;3. Prompt：我更关心“约束与可复现”，不关心花活&quot;">​</a></h2><p>我写 prompt 的习惯是把它当作“接口契约”：</p><ul><li>输入是什么（上下文、变量、约束）</li><li>输出是什么（格式、字段、允许为空的口径）</li><li>失败怎么处理（缺信息就问、无法判断就返回 unknown）</li></ul><p>关于思维链/分步推理：它在工程里确实有用，但我更倾向把它当作“让模型慢下来”的手段，而不是“保证正确”的手段。真正的正确性要靠：</p><ul><li>结构化输出 + 校验</li><li>多视角复核（自检、对照、回归用例）</li><li>外部事实绑定（引用、来源、可追溯）</li></ul><h2 id="_4-a2ui-对话-→-图谱编排-→-json-→-xui-→-最终产出" tabindex="-1">4. A2UI：对话 → 图谱编排 → JSON → XUI → 最终产出 <a class="header-anchor" href="#_4-a2ui-对话-→-图谱编排-→-json-→-xui-→-最终产出" aria-label="Permalink to &quot;4. A2UI：对话 → 图谱编排 → JSON → XUI → 最终产出&quot;">​</a></h2><p>我做过一类更偏“把语言变成交付物”的尝试：A2UI。</p><p>核心链路大致是：</p><ul><li>用户用自然语言描述意图</li><li>Agent 把意图拆成结构化任务（用 LangGraph 这类编排）</li><li>输出 JSON（schema 固定）</li><li>前端用 XUI（面向 UI 的组件协议）渲染成界面</li><li>再把界面/配置输出成更接近 Markdown/Spec 的表达，便于评审与复盘</li></ul><p>这里最关键的是：JSON 是工程的地基。它让你能做校验、回归、diff、回放，而不是把产物困在一段不可控的自然语言里。</p><h2 id="_5-可靠性与治理-从个人爽感到团队可控" tabindex="-1">5. 可靠性与治理：从个人爽感到团队可控 <a class="header-anchor" href="#_5-可靠性与治理-从个人爽感到团队可控" aria-label="Permalink to &quot;5. 可靠性与治理：从个人爽感到团队可控&quot;">​</a></h2><p>我越来越确信：2025 大概率是 Agent 元年，但难点不会在模型，而在治理。</p><ul><li>成本：token、延迟、失败重试、工具调用次数</li><li>风险：数据泄漏、合规与版权、依赖锁定、版本漂移</li><li>兜底：超时、降级、回滚、审计、责任边界</li></ul><p>如果你把 Agent 当成“能干活的实习生”，那治理就相当于：</p><ul><li>给它清晰的工作说明书（prompt/contract）</li><li>给它可检查的交付物格式（schema）</li><li>给它可回放的日志（trace）</li><li>给它可验证的回归用例（tests）</li></ul><h2 id="参考" tabindex="-1">参考 <a class="header-anchor" href="#参考" aria-label="Permalink to &quot;参考&quot;">​</a></h2><ul><li>LangGraph 文档 <a href="https://langchain-ai.github.io/langgraph/" target="_blank" rel="noreferrer">https://langchain-ai.github.io/langgraph/</a></li><li>ReAct: Synergizing Reasoning and Acting in Language Models (2022) <a href="https://arxiv.org/abs/2210.03629" target="_blank" rel="noreferrer">https://arxiv.org/abs/2210.03629</a></li></ul>',27))])}const b=i(p,[["render",h]]);export{I as __pageData,b as default};
