import{f as l,D as r,c as n,o as s,k as a,H as o,a5 as d,a as t}from"./chunks/framework.BsEAYNxL.js";const g=JSON.parse('{"title":"图像生成原理：扩散模型与 Latent Space","description":"","frontmatter":{"title":"图像生成原理：扩散模型与 Latent Space","date":"2026-01-21T00:00:00.000Z","categories":"study","tags":["ai-chitchat","diffusion","ddpm","ddim","ldm","dit"]},"headers":[],"relativePath":"ai-chitchat/02-diffusion-principle.md","filePath":"ai-chitchat/02-diffusion-principle.md","lastUpdated":1769663832000}'),u={name:"ai-chitchat/02-diffusion-principle.md"};function f(p,i,m,D,c,h){const e=r("InkMermaidBlock");return s(),n("div",null,[i[0]||(i[0]=a("div",{class:"alert read-stats",role:"note"},[a("span",{class:"read-stats__icon","aria-hidden":"true"},"🔖"),a("span",{class:"read-stats__text"},[t("本文约 "),a("b",null,"776"),t(" 字，阅读预计耗时 "),a("b",null,"2"),t(" 分钟。")])],-1)),i[1]||(i[1]=a("div",{class:"alert ai-disclosure",role:"note"},[a("span",{class:"ai-disclosure__icon","aria-hidden":"true"},"ⓘ"),a("span",{class:"ai-disclosure__text"},"AI 辅助写作声明：本文由博主构思逻辑，AI 辅助润色，双方共同校对。")],-1)),i[2]||(i[2]=a("p",null,"如果说 LLM 让人震惊的是“语言居然能被压缩成统计规律”，那扩散模型让我震惊的是：图像生成居然可以被讲成一个很朴素的工程过程——反复加噪、再反复去噪。",-1)),i[3]||(i[3]=a("p",null,"这篇我会尽量用类比去讲，但会把类比的失效边界标出来，避免你被比喻骗去做错误直觉。",-1)),i[4]||(i[4]=a("h2",{id:"_1-核心直觉-加噪与去噪",tabindex:"-1"},[t("1. 核心直觉：加噪与去噪 "),a("a",{class:"header-anchor",href:"#_1-核心直觉-加噪与去噪","aria-label":'Permalink to "1. 核心直觉：加噪与去噪"'},"​")],-1)),i[5]||(i[5]=a("p",null,"扩散模型的核心故事是：",-1)),i[6]||(i[6]=a("ul",null,[a("li",null,[a("strong",null,"正向过程"),t("：把一张干净图一步步加噪，最终变成纯噪声。")]),a("li",null,[a("strong",null,"反向过程"),t("：训练一个模型学会“每一步怎么去噪”，从纯噪声一步步还原出图像。")])],-1)),o(e,{codeBase64:"Zmxvd2NoYXJ0IExSCiAgWDBb5bmy5YeA5Zu+IHgwXSAtLT585Yqg5ZmqfCBYMVt4MV0KICBYMSAtLT585Yqg5ZmqfCBYVFt4VOKJiOe6r+WZquWjsF0KICBYVCAtLT585Y675Zmq5qih5Z6LIM61zrh8IFgxcFt4MSddCiAgWDFwIC0tPnzljrvlmarmqKHlnosgzrXOuHwgWDBwW3gwJ10K"}),i[7]||(i[7]=d('<p><strong>类比（擦黑板）</strong>：你先把黑板反复涂黑到看不清（加噪），再训练一个“擦黑板的手”学会每一步擦一点（去噪）。</p><h2 id="_2-ddpm-最朴素的加噪-去噪故事" tabindex="-1">2. DDPM：最朴素的加噪/去噪故事 <a class="header-anchor" href="#_2-ddpm-最朴素的加噪-去噪故事" aria-label="Permalink to &quot;2. DDPM：最朴素的加噪/去噪故事&quot;">​</a></h2><p>DDPM（Denoising Diffusion Probabilistic Models）的直觉：</p><ul><li>正向过程是人为定义的（通常是高斯噪声逐步注入）。</li><li>反向过程是学出来的：模型预测当前噪声（或等价量），再把它去掉一点点。</li></ul><p>你可以把 DDPM 看成“把一个很难的生成问题拆成 T 个小问题”。每个小问题都变得更像“修复”，而不是“一步生成”。</p><h2 id="_3-ldm-stable-diffusion-为什么要在-latent-里扩散" tabindex="-1">3. LDM / Stable Diffusion：为什么要在 Latent 里扩散 <a class="header-anchor" href="#_3-ldm-stable-diffusion-为什么要在-latent-里扩散" aria-label="Permalink to &quot;3. LDM / Stable Diffusion：为什么要在 Latent 里扩散&quot;">​</a></h2><p>如果直接在像素空间做扩散，成本会非常高：分辨率一上去，计算量和显存就爆炸。</p><p>Latent Diffusion 的关键取舍是：</p><ul><li>先用 VAE 把图像压到 <strong>Latent Space</strong>（更小、更结构化）。</li><li>在 Latent 里做扩散（更便宜）。</li><li>最后用 VAE 解码回像素。</li></ul><p><strong>类比（先把图片压成矢量草稿）</strong>：你不是在原图上擦擦改改，而是先把图压成“草稿”（Latent），在草稿层面修改，再导出成高清图。</p><h2 id="_4-加速与进化-ddim-与-dit" tabindex="-1">4. 加速与进化：DDIM 与 DiT <a class="header-anchor" href="#_4-加速与进化-ddim-与-dit" aria-label="Permalink to &quot;4. 加速与进化：DDIM 与 DiT&quot;">​</a></h2><h3 id="_4-1-ddim-跳步采样的魔法" tabindex="-1">4.1 DDIM：跳步采样的魔法 <a class="header-anchor" href="#_4-1-ddim-跳步采样的魔法" aria-label="Permalink to &quot;4.1 DDIM：跳步采样的魔法&quot;">​</a></h3><p>DDIM 的直觉是：在不改变训练模型的情况下，选择一种更接近确定性的采样路径，从而减少采样步数。 工程上，DDIM 往往能显著提速，但画风、多样性会受到采样策略影响。</p><h3 id="_4-2-dit-transformer-进场" tabindex="-1">4.2 DiT：Transformer 进场 <a class="header-anchor" href="#_4-2-dit-transformer-进场" aria-label="Permalink to &quot;4.2 DiT：Transformer 进场&quot;">​</a></h3><p>DiT（Diffusion Transformer）可以粗暴理解成：把传统的 U-Net 去噪器换成 Transformer。 工程直觉：</p><ul><li>Transformer 在建模全局关系上更直接。</li><li>模型规模化（Scaling）在 Transformer 家族上更成熟。</li></ul><h2 id="_5-你会在哪些地方翻车" tabindex="-1">5. 你会在哪些地方翻车 <a class="header-anchor" href="#_5-你会在哪些地方翻车" aria-label="Permalink to &quot;5. 你会在哪些地方翻车&quot;">​</a></h2><ul><li><strong>Prompt 失效</strong>：你以为模型在“理解”，其实它是在用统计关联补全你没说清的约束。</li><li><strong>参数打架</strong>：你以为步数越多越好，结果画面细节反而崩（采样器/CFG/噪声调度在互相打架）。</li><li><strong>VAE 伪影</strong>：你以为 Latent 是免费午餐，结果遇到 VAE 解码带来的纹理怪异或细节损失。</li></ul><h2 id="_6-总结" tabindex="-1">6. 总结 <a class="header-anchor" href="#_6-总结" aria-label="Permalink to &quot;6. 总结&quot;">​</a></h2><ul><li>扩散模型不是“一步生成”，而是“反复修复”。</li><li>Latent Diffusion 的本质是把生成从像素搬到更便宜的表示空间。</li><li>采样器与步数是工程旋钮：速度、质量、稳定性之间没有白嫖。</li></ul>',20))])}const b=l(u,[["render",f]]);export{g as __pageData,b as default};
