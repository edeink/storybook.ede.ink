import{f as t,D as r,c as s,o,k as i,H as n,a4 as d,a as e}from"./chunks/framework.BAs4fWuZ.js";const _=JSON.parse('{"title":"AI 碎碎念 02：扩散模型从 DDPM 到 DiT（用类比讲清楚加噪与去噪）","description":"","frontmatter":{"title":"AI 碎碎念 02：扩散模型从 DDPM 到 DiT（用类比讲清楚加噪与去噪）","date":"2026-01-21T00:00:00.000Z","categories":"study","tags":["ai-chitchat","diffusion","ddpm","ddim","ldm","stable-diffusion","dit"]},"headers":[],"relativePath":"ai-chitchat/02-diffusion-models-primer.md","filePath":"ai-chitchat/02-diffusion-models-primer.md","lastUpdated":1769026429000}'),f={name:"ai-chitchat/02-diffusion-models-primer.md"};function u(p,a,m,h,D,b){const l=r("InkMermaidBlock");return o(),s("div",null,[a[0]||(a[0]=i("div",{class:"alert read-stats",role:"note"},[i("span",{class:"read-stats__icon","aria-hidden":"true"},"🔖"),i("span",{class:"read-stats__text"},[e("本文约 "),i("b",null,"1123"),e(" 字，阅读预计耗时 "),i("b",null,"3"),e(" 分钟。")])],-1)),a[1]||(a[1]=i("div",{class:"alert ai-disclosure",role:"note"},[i("span",{class:"ai-disclosure__icon","aria-hidden":"true"},"ⓘ"),i("span",{class:"ai-disclosure__text"},"AI 辅助写作声明：本文由博主构思逻辑，AI 辅助润色，双方共同校对。")],-1)),a[2]||(a[2]=i("p",null,"如果说 LLM 让人震惊的是“语言居然能被压缩成统计规律”，那扩散模型让我震惊的是：图像生成居然可以被讲成一个很朴素的工程过程——反复加噪、再反复去噪。",-1)),a[3]||(a[3]=i("p",null,"这篇我会尽量用类比去讲，但会把类比的失效边界标出来，避免你被比喻骗去做错误直觉。",-1)),a[4]||(a[4]=i("h2",{id:"_1-一句话版本-扩散模型在做什么",tabindex:"-1"},[e("1. 一句话版本：扩散模型在做什么 "),i("a",{class:"header-anchor",href:"#_1-一句话版本-扩散模型在做什么","aria-label":'Permalink to "1. 一句话版本：扩散模型在做什么"'},"​")],-1)),a[5]||(a[5]=i("p",null,"扩散模型的核心故事是：",-1)),a[6]||(a[6]=i("ul",null,[i("li",null,"正向过程：把一张干净图一步步加噪，最终变成纯噪声"),i("li",null,"反向过程：训练一个模型学会“每一步怎么去噪”，从纯噪声一步步还原出图像")],-1)),n(l,{codeBase64:"Zmxvd2NoYXJ0IExSCiAgWDBb5bmy5YeA5Zu+IHgwXSAtLT585Yqg5ZmqfCBYMVt4MV0KICBYMSAtLT585Yqg5ZmqfCBYVFt4VOKJiOe6r+WZquWjsF0KICBYVCAtLT585Y675Zmq5qih5Z6LIM61zrh8IFgxcFt4MSddCiAgWDFwIC0tPnzljrvlmarmqKHlnosgzrXOuHwgWDBwW3gwJ10K"}),a[7]||(a[7]=d('<p>类比 1（擦黑板）：你先把黑板反复涂黑到看不清（加噪），再训练一个“擦黑板的手”学会每一步擦一点（去噪）。</p><p>类比失效边界：真实的“擦黑板”是确定性的，而扩散的反向过程通常带随机性与采样策略；此外“擦回来的图”不一定是原图，而是从分布里采样的一张“看起来合理的图”。</p><h2 id="_2-ddpm-最朴素的加噪-去噪故事" tabindex="-1">2. DDPM：最朴素的加噪/去噪故事 <a class="header-anchor" href="#_2-ddpm-最朴素的加噪-去噪故事" aria-label="Permalink to &quot;2. DDPM：最朴素的加噪/去噪故事&quot;">​</a></h2><p>DDPM（Denoising Diffusion Probabilistic Models）的直觉：</p><ul><li>正向过程是人为定义的（通常是高斯噪声逐步注入）</li><li>反向过程是学出来的：模型预测当前噪声（或等价量），再把它去掉一点点</li></ul><p>你可以把 DDPM 看成“把一个很难的生成问题拆成 T 个小问题”。每个小问题都变得更像“修复”，而不是“一步生成”。</p><h2 id="_3-ldm-stable-diffusion-为什么要在-latent-里扩散" tabindex="-1">3. LDM / Stable Diffusion：为什么要在 latent 里扩散 <a class="header-anchor" href="#_3-ldm-stable-diffusion-为什么要在-latent-里扩散" aria-label="Permalink to &quot;3. LDM / Stable Diffusion：为什么要在 latent 里扩散&quot;">​</a></h2><p>如果直接在像素空间做扩散，成本会非常高：分辨率一上去，计算量和显存就爆炸。</p><p>Latent Diffusion 的关键取舍是：</p><ul><li>先用 VAE 把图像压到 latent space（更小、更结构化）</li><li>在 latent 里做扩散（更便宜）</li><li>最后用 VAE 解码回像素</li></ul><p>类比 2（先把图片压成矢量草稿）：你不是在原图上擦擦改改，而是先把图压成“草稿”（latent），在草稿层面修改，再导出成高清图。</p><p>类比失效边界：latent 并不是人类可直接理解的矢量草稿，它只是一个可学习的压缩空间；压缩会损失信息，这也是很多生成细节“糊”或“怪”的来源之一。</p><h2 id="_4-ddim-为什么能更快-以及牺牲了什么" tabindex="-1">4. DDIM：为什么能更快（以及牺牲了什么） <a class="header-anchor" href="#_4-ddim-为什么能更快-以及牺牲了什么" aria-label="Permalink to &quot;4. DDIM：为什么能更快（以及牺牲了什么）&quot;">​</a></h2><p>DDIM 常被理解为“用更少的步数采样”。它的直觉是：在不改变训练模型的情况下，选择一种更接近确定性的采样路径，从而减少采样步数。</p><p>工程上，你关心的是：</p><ul><li>DDIM 往往能显著提速</li><li>但画风、多样性、稳定性会受到采样策略影响</li><li>“步数”和“质量”不是单调关系：质量、细节、稳定性、速度之间是 trade-off</li></ul><h2 id="_5-dit-为什么大家开始用-transformer-当去噪器" tabindex="-1">5. DiT：为什么大家开始用 Transformer 当去噪器 <a class="header-anchor" href="#_5-dit-为什么大家开始用-transformer-当去噪器" aria-label="Permalink to &quot;5. DiT：为什么大家开始用 Transformer 当去噪器&quot;">​</a></h2><p>DiT（Diffusion Transformer）可以粗暴理解成：把传统的 U-Net 去噪器换成 Transformer。</p><p>这背后的工程直觉是：</p><ul><li>Transformer 在建模全局关系上更直接</li><li>模型规模化（scaling）在 Transformer 家族上更成熟</li><li>代价是：计算图、显存占用、推理速度等维度会重新洗牌</li></ul><p>我更关心的不是“谁又刷新了榜”，而是：它把复杂度挪到了哪里，让哪些工程问题变得更尖锐。</p><h2 id="_6-你会在哪些地方翻车" tabindex="-1">6. 你会在哪些地方翻车 <a class="header-anchor" href="#_6-你会在哪些地方翻车" aria-label="Permalink to &quot;6. 你会在哪些地方翻车&quot;">​</a></h2><ul><li>你以为模型在“理解 prompt”，其实它是在用统计关联补全你没说清的约束</li><li>你以为步数越多越好，结果画面细节反而崩（采样器/CFG/噪声调度在互相打架）</li><li>你以为 latent 是免费午餐，结果遇到 VAE 解码带来的纹理怪异或细节损失</li><li>你以为换个更大的模型就能解决，结果发现问题根在数据分布与评估口径</li></ul><h2 id="_7-这一章你应该带走的三句话" tabindex="-1">7. 这一章你应该带走的三句话 <a class="header-anchor" href="#_7-这一章你应该带走的三句话" aria-label="Permalink to &quot;7. 这一章你应该带走的三句话&quot;">​</a></h2><ul><li>扩散模型不是“一步生成”，而是“反复修复”：加噪把问题拆小，去噪把分布学回来</li><li>latent diffusion 的本质是把生成从像素搬到更便宜的表示空间，但它有压缩损失</li><li>采样器与步数是工程旋钮：速度、质量、稳定性之间没有白嫖</li></ul><h2 id="参考" tabindex="-1">参考 <a class="header-anchor" href="#参考" aria-label="Permalink to &quot;参考&quot;">​</a></h2><ul><li>DDPM: Ho et al. Denoising Diffusion Probabilistic Models (2020) <a href="https://arxiv.org/abs/2006.11239" target="_blank" rel="noreferrer">https://arxiv.org/abs/2006.11239</a></li><li>DDIM: Song et al. Denoising Diffusion Implicit Models (2020) <a href="https://arxiv.org/abs/2010.02502" target="_blank" rel="noreferrer">https://arxiv.org/abs/2010.02502</a></li><li>Latent Diffusion / Stable Diffusion: Rombach et al. High-Resolution Image Synthesis with Latent Diffusion Models (2022) <a href="https://arxiv.org/abs/2112.10752" target="_blank" rel="noreferrer">https://arxiv.org/abs/2112.10752</a></li><li>DiT: Peebles, Xie. Scalable Diffusion Models with Transformers (2022) <a href="https://arxiv.org/abs/2212.09748" target="_blank" rel="noreferrer">https://arxiv.org/abs/2212.09748</a></li></ul>',27))])}const g=t(f,[["render",u]]);export{_ as __pageData,g as default};
