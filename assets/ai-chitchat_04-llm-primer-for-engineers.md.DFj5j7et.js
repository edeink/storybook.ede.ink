import{_ as n,C as s,c as d,o as r,j as a,a2 as l,b as p,a as t,w as o,G as m,a3 as u}from"./chunks/framework.C2eRlmqf.js";const B=JSON.parse('{"title":"AI 碎碎念 04：LLM 够用版原理（Transformer、上下文与概率）","description":"","frontmatter":{"title":"AI 碎碎念 04：LLM 够用版原理（Transformer、上下文与概率）","date":"2026-01-21T00:00:00.000Z","categories":"study","tags":["ai-chitchat","llm","transformer","attention","token"]},"headers":[],"relativePath":"ai-chitchat/04-llm-primer-for-engineers.md","filePath":"ai-chitchat/04-llm-primer-for-engineers.md","lastUpdated":1769026429000}'),c={name:"ai-chitchat/04-llm-primer-for-engineers.md"};function h(_,e,E,A,b,f){const i=s("Mermaid");return r(),d("div",null,[e[1]||(e[1]=a("div",{class:"alert read-stats",role:"note"},[a("span",{class:"read-stats__icon","aria-hidden":"true"},"🔖"),a("span",{class:"read-stats__text"},[t("本文约 "),a("b",null,"895"),t(" 字，阅读预计耗时 "),a("b",null,"3"),t(" 分钟。")])],-1)),e[2]||(e[2]=a("div",{class:"alert ai-disclosure",role:"note"},[a("span",{class:"ai-disclosure__icon","aria-hidden":"true"},"ⓘ"),a("span",{class:"ai-disclosure__text"},"AI 辅助写作声明：本文由博主构思逻辑，AI 辅助润色，双方共同校对。")],-1)),e[3]||(e[3]=l('<p>我对 LLM 的理解一直偏“科普 + 工程直觉”：它能做什么、不能做什么、贵在哪里、怎么不翻车。</p><p>这篇不讲训练细节，不碰复杂推导，只把几个你绕不开的开关讲清楚：Transformer、token、上下文窗口、以及“它本质是概率学”。</p><h2 id="_1-一句话-llm-在做条件概率的下一步预测" tabindex="-1">1. 一句话：LLM 在做条件概率的下一步预测 <a class="header-anchor" href="#_1-一句话-llm-在做条件概率的下一步预测" aria-label="Permalink to &quot;1. 一句话：LLM 在做条件概率的下一步预测&quot;">​</a></h2><p>把一句很宏大的话说得很土：</p><ul><li>给定上下文（已经出现的 token 序列）</li><li>模型输出“下一个 token 的概率分布”</li><li>你从分布里采样/取最大概率，得到下一个 token</li><li>重复这个过程，就得到一段文本</li></ul><p>所以它看起来像“理解”，但底层是“统计规律 + 大规模拟合”。工程上最重要的一点是：它的输出并不是事实保证，而是概率意义上的“合理续写”。</p><h2 id="_2-token-你以为它读的是字-其实它读的是-token" tabindex="-1">2. Token：你以为它读的是字，其实它读的是 token <a class="header-anchor" href="#_2-token-你以为它读的是字-其实它读的是-token" aria-label="Permalink to &quot;2. Token：你以为它读的是字，其实它读的是 token&quot;">​</a></h2><p>tokenization 会把文本切成 token。token 不是字，也不是词，它是分词算法在“压缩效率与表达能力”之间做的取舍结果。</p><p>工程后果：</p><ul><li>你的输入长度以 token 计价（成本/延迟）</li><li>同一句话换一种写法，token 数可能差很多</li><li>“边界条件写得太长”会直接把上下文窗口撑爆</li></ul><h2 id="_3-transformer-attention-是-相关性路由" tabindex="-1">3. Transformer：attention 是“相关性路由” <a class="header-anchor" href="#_3-transformer-attention-是-相关性路由" aria-label="Permalink to &quot;3. Transformer：attention 是“相关性路由”&quot;">​</a></h2><p>attention 的直觉可以理解成：在一段序列里，每个位置都可以“去看”其他位置里哪些信息与自己相关。</p>',12)),(r(),p(u,null,{default:o(()=>[m(i,{id:"mermaid-71",class:"mermaid",graph:"flowchart%20TB%0A%20%20X%5B%E8%BE%93%E5%85%A5%20token%20embeddings%5D%20--%3E%20A%5B%E5%A4%9A%E5%A4%B4%E6%B3%A8%E6%84%8F%E5%8A%9B%3Cbr%2F%3E%E6%89%BE%E7%9B%B8%E5%85%B3%E6%80%A7%5D%0A%20%20A%20--%3E%20F%5B%E5%89%8D%E9%A6%88%E7%BD%91%E7%BB%9C%3Cbr%2F%3E%E9%9D%9E%E7%BA%BF%E6%80%A7%E5%8F%98%E6%8D%A2%5D%0A%20%20F%20--%3E%20X2%5B%E8%BE%93%E5%87%BA%E8%A1%A8%E7%A4%BA%5D%0A%20%20X2%20--%3E%7C%E5%A0%86%E5%8F%A0%20N%20%E5%B1%82%7C%20Y%5B%E6%9B%B4%E9%AB%98%E5%B1%82%E8%A1%A8%E7%A4%BA%5D%0A"})]),fallback:o(()=>[...e[0]||(e[0]=[t(" Loading... ",-1)])]),_:1})),e[4]||(e[4]=l('<p>你不需要背 Q/K/V 的公式，但你要知道 attention 带来的工程直觉：</p><ul><li>它擅长建模长距离依赖（比纯 RNN 更直接）</li><li>它的成本与上下文长度强相关（长上下文更贵）</li><li>它并不会自动“更可靠”，它只是“更能利用上下文”</li></ul><h2 id="_4-编解码结构-encoder-decoder-vs-decoder-only" tabindex="-1">4. 编解码结构：encoder-decoder vs decoder-only <a class="header-anchor" href="#_4-编解码结构-encoder-decoder-vs-decoder-only" aria-label="Permalink to &quot;4. 编解码结构：encoder-decoder vs decoder-only&quot;">​</a></h2><p>最简版本：</p><ul><li>encoder-decoder：输入一段、输出一段（翻译/摘要等经典任务）</li><li>decoder-only：不断预测下一个 token（多数现代 LLM 属于这类）</li></ul><p>你不需要纠结结构名词，工程上只需要知道：不同结构对应不同的使用姿势与工具链生态。</p><h2 id="_5-embedding-latent-向量空间不是玄学" tabindex="-1">5. Embedding / Latent：向量空间不是玄学 <a class="header-anchor" href="#_5-embedding-latent-向量空间不是玄学" aria-label="Permalink to &quot;5. Embedding / Latent：向量空间不是玄学&quot;">​</a></h2><p>embedding 可以看作“把符号变成向量”的入口；latent space 则是“压缩后的表示空间”。</p><p>LLM 里你会经常听到“向量语义相似”，它的工程意义是：</p><ul><li>相似的 token/片段会在向量空间里靠近</li><li>你可以在向量空间里做检索、聚类、路由</li></ul><p>但也别神化：相似性不是逻辑等价，更不是事实一致。</p><h2 id="_6-采样-temperature-top-p-是-输出性格旋钮" tabindex="-1">6. 采样：temperature/top-p 是“输出性格旋钮” <a class="header-anchor" href="#_6-采样-temperature-top-p-是-输出性格旋钮" aria-label="Permalink to &quot;6. 采样：temperature/top-p 是“输出性格旋钮”&quot;">​</a></h2><ul><li>temperature 更像“敢不敢冒险”：越高越发散，越低越保守</li><li>top-p 更像“把概率尾巴截掉”：只在累计概率达到 p 的候选里选</li></ul><p>工程上常见误用是：把温度调太高导致输出漂移，再用更长 prompt 去补救，结果上下文更长、成本更高、漂移更严重。</p><h2 id="_7-幻觉-不是-bug-是默认行为" tabindex="-1">7. 幻觉：不是 bug，是默认行为 <a class="header-anchor" href="#_7-幻觉-不是-bug-是默认行为" aria-label="Permalink to &quot;7. 幻觉：不是 bug，是默认行为&quot;">​</a></h2><p>LLM 的幻觉更像“在不知道的时候依然要给出一个听起来合理的续写”。所以你要把它当成一个默认会胡说的系统，再通过机制把胡说压下去。</p><p>工程兜底策略通常是：</p><ul><li>结构化输出（schema/JSON）</li><li>引用与可追溯（要求给出处，或把结论绑定到来源）</li><li>评估与回归（用固定用例测一致性与正确性）</li></ul><h2 id="参考" tabindex="-1">参考 <a class="header-anchor" href="#参考" aria-label="Permalink to &quot;参考&quot;">​</a></h2><ul><li>Attention Is All You Need（Transformer 原论文）<a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noreferrer">https://arxiv.org/abs/1706.03762</a></li><li>The Illustrated Transformer（直觉图解）<a href="https://jalammar.github.io/illustrated-transformer/" target="_blank" rel="noreferrer">https://jalammar.github.io/illustrated-transformer/</a></li></ul>',20))])}const g=n(c,[["render",h]]);export{B as __pageData,g as default};
