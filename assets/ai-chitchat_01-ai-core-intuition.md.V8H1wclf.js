import{f as i,D as o,c as r,o as p,k as s,a4 as t,H as l,a as n}from"./chunks/framework.BAs4fWuZ.js";const x=JSON.parse('{"title":"AI 核心直觉：从梯度下降到 Transformer","description":"","frontmatter":{"title":"AI 核心直觉：从梯度下降到 Transformer","date":"2026-01-21T00:00:00.000Z","categories":"study","tags":["ai-chitchat","ml","dl","llm","transformer","intuition"]},"headers":[],"relativePath":"ai-chitchat/01-ai-core-intuition.md","filePath":"ai-chitchat/01-ai-core-intuition.md","lastUpdated":1769663832000}'),m={name:"ai-chitchat/01-ai-core-intuition.md"};function c(h,a,d,g,u,_){const e=o("InkMermaidBlock");return p(),r("div",null,[a[0]||(a[0]=s("div",{class:"alert read-stats",role:"note"},[s("span",{class:"read-stats__icon","aria-hidden":"true"},"🔖"),s("span",{class:"read-stats__text"},[n("本文约 "),s("b",null,"1174"),n(" 字，阅读预计耗时 "),s("b",null,"3"),n(" 分钟。")])],-1)),a[1]||(a[1]=s("div",{class:"alert ai-disclosure",role:"note"},[s("span",{class:"ai-disclosure__icon","aria-hidden":"true"},"ⓘ"),s("span",{class:"ai-disclosure__text"},"AI 辅助写作声明：本文由博主构思逻辑，AI 辅助润色，双方共同校对。")],-1)),a[2]||(a[2]=t('<p>我从 2023 年开始陆陆续续学 AI，2024 年在电商里下场折腾。但如果从完整 AI 体系看，我依旧是“勺子”水平。</p><p>这篇不装专业，也不试图把你推去当算法研究员。我只想把几个绕不开的概念讲成“工程直觉”：你听完能知道它大概在干什么、贵在哪里、会在哪些地方翻车。</p><h2 id="_1-ai-的本质-一个可训练的函数近似器" tabindex="-1">1. AI 的本质：一个可训练的函数近似器 <a class="header-anchor" href="#_1-ai-的本质-一个可训练的函数近似器" aria-label="Permalink to &quot;1. AI 的本质：一个可训练的函数近似器&quot;">​</a></h2><p>多数你今天看到的 AI（尤其是深度学习）都可以被压缩成一个句子：</p><ul><li>你给一堆输入 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">x</span></span></span></span>（图、文、特征）</li><li>你希望它输出 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span></span></span>（分类、回归、下一 token、噪声残差）</li><li>你定义一个损失函数 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mo stretchy="false">(</mo><mover accent="true"><mi>y</mi><mo>^</mo></mover><mo separator="true">,</mo><mi>y</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">L(\\hat{y}, y)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">L</span><span class="mopen">(</span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6944em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.1944em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1944em;"><span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mclose">)</span></span></span></span> 来衡量“错多少”</li><li>你用梯度下降把参数往“让 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi></mrow><annotation encoding="application/x-tex">L</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal">L</span></span></span></span> 变小”的方向挪</li></ul><p>工程上，AI 的核心并不神秘：它是一个可训练的函数近似器。它强在表达能力与规模，弱在可解释性与稳定性。</p>',6)),l(e,{codeBase64:"Zmxvd2NoYXJ0IExSCiAgRFvmlbDmja4geCx5XSAtLT4gTVvmqKHlnosgZih4OyDOuCldCiAgTSAtLT4gWUhb6aKE5rWLIHnMgl0KICBZSCAtLT4gTFvmjZ/lpLEgTCh5zIIseSldCiAgTCAtLT4gR1vmoq/luqYg4oiCTC/iiILOuF0KICBHIC0tPiBVW+abtOaWsCDOuF0KICBVIC0tPiBNCg=="}),a[3]||(a[3]=t('<p><strong>传统机器学习 vs 深度学习：</strong></p><ul><li><strong>传统 ML</strong>：你负责特征工程（把世界变成特征），模型负责拟合。</li><li><strong>Deep Learning</strong>：你把特征提取也交给模型，代价是数据与算力需求暴涨。</li></ul><h2 id="_2-核心机制-梯度下降与反向传播" tabindex="-1">2. 核心机制：梯度下降与反向传播 <a class="header-anchor" href="#_2-核心机制-梯度下降与反向传播" aria-label="Permalink to &quot;2. 核心机制：梯度下降与反向传播&quot;">​</a></h2><h3 id="_2-1-梯度下降-下山的艺术" tabindex="-1">2.1 梯度下降：下山的艺术 <a class="header-anchor" href="#_2-1-梯度下降-下山的艺术" aria-label="Permalink to &quot;2.1 梯度下降：下山的艺术&quot;">​</a></h3><p>梯度下降的直觉可以用一个很土的类比：</p><ul><li>损失函数像地形高度。</li><li>你站在山上想下到谷底。</li><li>梯度就是“往哪边下坡最快”。</li></ul><p>这里的 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>η</mi></mrow><annotation encoding="application/x-tex">\\eta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">η</span></span></span></span>（学习率）就很像“步子迈多大”。步子太大容易摔死（发散），步子太小容易磨蹭到天荒地老（收敛慢）。</p><p>工程上，优化问题经常不是“会不会算梯度”，而是：</p><ul><li>你的损失定义对不对（是否真的在优化你想要的指标）</li><li>你的数据分布对不对（是否泄漏、是否和线上一致）</li></ul><h3 id="_2-2-反向传播-高效对账" tabindex="-1">2.2 反向传播：高效对账 <a class="header-anchor" href="#_2-2-反向传播-高效对账" aria-label="Permalink to &quot;2.2 反向传播：高效对账&quot;">​</a></h3><p>反向传播（Backpropagation）本质是链式法则的工程化。你可以把它理解成“对账”：</p><ul><li><strong>前向</strong>：算出结果，得到“欠账”（损失）。</li><li><strong>反向</strong>：把欠账按责任分摊到每一层参数上。</li></ul><p>它不是“AI 的秘密武器”，它只是让“可微分的模型”能被大规模训练。</p><h2 id="_3-通用概念-embedding-与-latent-space" tabindex="-1">3. 通用概念：Embedding 与 Latent Space <a class="header-anchor" href="#_3-通用概念-embedding-与-latent-space" aria-label="Permalink to &quot;3. 通用概念：Embedding 与 Latent Space&quot;">​</a></h2><h3 id="_3-1-embedding-把离散符号变成向量" tabindex="-1">3.1 Embedding：把离散符号变成向量 <a class="header-anchor" href="#_3-1-embedding-把离散符号变成向量" aria-label="Permalink to &quot;3.1 Embedding：把离散符号变成向量&quot;">​</a></h3><p>文字、类目、ID 都是离散符号，模型没法直接算。Embedding 做的事就是：把离散符号映射到一个向量空间里，让“相似性”有机会被学习出来。</p><p>工程上你只需要记住一句话：Embedding 是模型的“词典”，坏 Embedding 会让模型一开始就听不懂你在说什么。</p><h3 id="_3-2-latent-space-压缩后的表达" tabindex="-1">3.2 Latent Space：压缩后的表达 <a class="header-anchor" href="#_3-2-latent-space-压缩后的表达" aria-label="Permalink to &quot;3.2 Latent Space：压缩后的表达&quot;">​</a></h3><p>Latent Space（潜在空间）可以理解成一种“更小、更结构化的压缩表示”。 它常见于生成模型：先把像素压进 Latent，再在 Latent 里做复杂计算（更便宜），最后再解码回像素。你会在扩散模型里反复见到它。</p><h2 id="_4-llm-革命-transformer-与概率预测" tabindex="-1">4. LLM 革命：Transformer 与概率预测 <a class="header-anchor" href="#_4-llm-革命-transformer-与概率预测" aria-label="Permalink to &quot;4. LLM 革命：Transformer 与概率预测&quot;">​</a></h2><p>LLM 的本质是在做 <strong>条件概率的下一步预测</strong>。它看起来像“理解”，但底层是“统计规律 + 大规模拟合”。</p><h3 id="_4-1-token-模型读的不是字" tabindex="-1">4.1 Token：模型读的不是字 <a class="header-anchor" href="#_4-1-token-模型读的不是字" aria-label="Permalink to &quot;4.1 Token：模型读的不是字&quot;">​</a></h3><p>Token 是分词算法在“压缩效率与表达能力”之间做的取舍结果。 工程后果：</p><ul><li>你的输入长度以 Token 计价（成本/延迟）。</li><li>“边界条件写得太长”会直接把上下文窗口撑爆。</li></ul><h3 id="_4-2-transformer-attention-是-相关性路由" tabindex="-1">4.2 Transformer：Attention 是“相关性路由” <a class="header-anchor" href="#_4-2-transformer-attention-是-相关性路由" aria-label="Permalink to &quot;4.2 Transformer：Attention 是“相关性路由”&quot;">​</a></h3><p>Attention 的直觉可以理解成：在一段序列里，每个位置都可以“去看”其他位置里哪些信息与自己相关。</p>',26)),l(e,{codeBase64:"Zmxvd2NoYXJ0IFRCCiAgWFvovpPlhaUgdG9rZW4gZW1iZWRkaW5nc10gLS0+IEFb5aSa5aS05rOo5oSP5YqbPGJyLz7mib7nm7jlhbPmgKddCiAgQSAtLT4gRlvliY3ppojnvZHnu5w8YnIvPumdnue6v+aAp+WPmOaNol0KICBGIC0tPiBYMlvovpPlh7rooajnpLpdCiAgWDIgLS0+fOWghuWPoCBOIOWxgnwgWVvmm7Tpq5jlsYLooajnpLpdCg=="}),a[4]||(a[4]=t('<p>你不需要背 Q/K/V 的公式，但你要知道 Attention 带来的工程直觉：</p><ul><li>它擅长建模长距离依赖。</li><li>它的成本与上下文长度强相关（长上下文更贵）。</li></ul><h3 id="_4-3-幻觉-不是-bug-是默认行为" tabindex="-1">4.3 幻觉：不是 bug，是默认行为 <a class="header-anchor" href="#_4-3-幻觉-不是-bug-是默认行为" aria-label="Permalink to &quot;4.3 幻觉：不是 bug，是默认行为&quot;">​</a></h3><p>LLM 的幻觉更像“在不知道的时候依然要给出一个听起来合理的续写”。所以你要把它当成一个默认会胡说的系统，再通过机制把胡说压下去。</p><p>工程兜底策略通常是：</p><ul><li><strong>结构化输出</strong>（Schema/JSON）</li><li><strong>引用与可追溯</strong>（要求给出处）</li><li><strong>评估与回归</strong>（用固定用例测一致性）</li></ul><h2 id="_5-总结" tabindex="-1">5. 总结 <a class="header-anchor" href="#_5-总结" aria-label="Permalink to &quot;5. 总结&quot;">​</a></h2><p>无论是基础的 ML 还是前沿的 LLM，它们在工程上都遵循类似的物理定律：</p><ul><li>数据质量决定上限（Garbage In, Garbage Out）。</li><li>评估口径决定方向（你优化的到底是什么）。</li><li>成本与效果永远在 Trade-off（Latency vs Quality）。</li></ul>',9))])}const L=i(m,[["render",c]]);export{x as __pageData,L as default};
